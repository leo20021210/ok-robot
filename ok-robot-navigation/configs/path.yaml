# debug=True: generate localization result for each text query; debug=False: communicate with robots and send planned path to robots
debug: True
version: 2

#"conservative_vlmap" (space not scanned is non-navigable) "brave_vlmap" (sapce scanned is navigable, used when you forget to scan the floor)
map_type: "conservative_vlmap" # "conservative_vlmap" "brave_vlmap"

# uint: meters (m)
# Considering the robot has volume, we have occ_avoid_radius, so that the points close to obstacles would be non-navigable
occ_avoid_radius: 0.2
# resolution of obstacle map
resolution: 0.1

# zmq communication port, need to the the same with robot's navigation port number
port_number: 5555

# planned path and generated localization results will be saved here
save_file: "./test"

# min (floor) and max (ceil) height for building obstalce map
min_height: -0.8
# max_height = min_height + 1.5 (check path_planning.py for more details)

# We provide very-easy-to-understand navigation visualization in a pointcloud opened in open3d, nevertheless
# many machines, especially headless machines might not be compatible with this visualization, for them the only
# visualization they can see is a blue-and-yellow obstacle map with path drawn on it, set pointcloud_visualization to 
# False if you do not want to check pointcloud visualization and only want to see the 2D obstacle map
pointcloud_visualization: True

# Parameters needed for loading semantic memory
cache_path: 'chuanyang.pt'
pointcloud_path: 'pointcloud.ply'
dataset_path: "/data/peiqi/r3d/LeoFriendKitchen.r3d"

# devices for loading semantic memory, we have a device for fetching semantic memory 
# and a device for running localization and path planning
# Usually they are both set to cuda, but in some GPU-limited situations, people are welcome to 
# fetch semantic memory with cuda and running path planning with cpu
memory_load_device: 'cuda'
path_planning_device: 'cuda'

# Data loading and labelling specs
sample_freq: 10 # sample frames from record3D file with this fps
threshold: 0.15 # owl-vit confidence threshold
subsample_prob: 0.005 # sampling probability for raw points (usually we will have millions of points from record3D file)

# Debug purposes, visualize OWL-ViT results
visualize_results: true
visualization_path: "sample_debug"

web_models:
  # OWL-ViT's config
  owl: "google/owlvit-base-patch32"
  # SAM's config
  sam: "vit_b"

# Add any custom labels you want here
#custom_labels:
#  - muffins
#  - hat
#  - pepper

# Or just comment it out for default labels.
custom_labels: null


